# **Behavioral Cloning** 

## Behavioral Cloning Project

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report

[//]: # (Image References)

[image1]: ./images_for_report/centre.jpg "Centre camera image from centre lane driving"
[image2]: ./images_for_report/left.jpg "Left camera image from centre lane driving"
[image3]: ./images_for_report/right.jpg "Right camera image from centre lane driving"

## Simulator

The simulator used for generating training data and testing the model is [Udacity's Self-Driving Car Simulator](https://github.com/udacity/self-driving-car-sim
)

## Files

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup_report.md or writeup_report.pdf summarizing the results

Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```

## Model Architecture and Training Strategy

### Data Generation and Preprocessing

Training and Validation data was generated by using centre lane driving. Three clockwise and one counter-clockwise lap of Track 1 was recorded with great care to avoid showing the car steering away from the centre lane. 

![alt text][image1]

Data showing recovery from the left and right sides of the road were trialed, but deemed unecessary. 

The data was augmented by flipping the images, and using the left and right camera images using a 0.2 correction angle to the steering angle. 

![alt text][image2]
![alt text][image3]

After collection, I had X number of data points, of which 20% was used for validation. 

The input 160x320x3 camera image data was cropped above and below to remove parts of the image above the road and parts that show sections of the car. These were thought to be irrelevant to the performance of driving. The image was then normalised using a Keras lambda layer.  


### Model

My model consists of a convolutional neural network inspired by [NVIDIA's End-to-End Deep Learning Architecture](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/). 

My final model consisted of the following layers:

| Layer         		|     Description	        					| 
|:---------------------:|:---------------------------------------------:| 
| Input         		| 160x320x3 RGB image   						| 
| Crop  	       		| 65x320x3 RGB image   							| 
| Convolution 5x5     	| 2x2 stride, valid padding, outputs 31x159x24 	|
| RELU					|												|
| Dropout				| dropout_prob									| 
| Convolution 5x5     	| 2x2 stride, valid padding, outputs 14x78x36 	|
| RELU					|												|
| Dropout				| dropout_prob									|
| Convolution 5x5     	| 2x2 stride, valid padding, outputs 6x38x48 	|
| RELU					|												|
| Dropout				| dropout_prob									|
| Convolution 3x3     	| 1x1 stride, valid padding, outputs 4x36x36 	|
| RELU					|												|
| Dropout				| dropout_prob									|
| Convolution 3x3     	| 1x1 stride, valid padding, outputs 2x34x64 	|
| RELU					|												|
| Dropout				| dropout_prob									|
| Flatten		      	| outputs 2x34x64 = 4352						|
| Fully connected		| outputs 100 									|
| Fully connected		| outputs 50  									|
| Fully connected		| outputs 10  									|
| Fully connected		| outputs 1  									|
| MSE					| 	        									|

Convolution layers have been used for different levels of feature extraction. ReLUs have been used to introduce non-linearities in the model. Dropout layers with a dropout probability of 0.5 were used for training to avoid overfitting to the data. 

An adam optimiser to choose learning parameters to reduce MSE when trained on data generated by a human operator. 

When completed, the model was tested on the simulator in "automated mode".

Two epochs were used in training as beyond this, the validation error seemed to increase, suggesting overfitting. 

## Solution Design Approach

The strategy I used for deriving a model architecture was start with a simple model architecture and iterate with the goal of reducing mean squared error for the validation set. 

My first step was to try a LeNet network that I had previously used to classify German traffic signs. This was able to drive to some degree, but was unable to complete the course. 

NVIDIA demonstrated results with an end-to-end convolutional architecture for self driving car applications. Trialling this with the same data showed improvements in mean squared error. Indeed it was almost successful in driving around track one. There were a few spots where the vehicle drove strayed away from the centre line. 

To particularly target these areas, I drove the car around the course a few more times, taking particular care to drive accurately and slowly around those particular regions to collect more data. In addition, as suggested in the paper, to prevent a bias of driving forward, 90% of data containing no steering input (steering angle of 0) were removed (line 49 in model.py). 






